import { useState, useRef, useCallback, useEffect } from 'react'
import { useUserStore } from '../stores/userStore'

const API_URL = import.meta.env.VITE_API_URL || '/api/v1'

// Check Web Speech API support
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition

export function useSpeechRecognition() {
  const [isRecording, setIsRecording] = useState(false)
  const [audioBlob, setAudioBlob] = useState(null)
  const [error, setError] = useState(null)
  const [result, setResult] = useState(null)
  const [isAnalyzing, setIsAnalyzing] = useState(false)

  // Real-time STT transcript (Fast Track)
  const [liveTranscript, setLiveTranscript] = useState('')
  const [finalTranscript, setFinalTranscript] = useState('')

  const mediaRecorderRef = useRef(null)
  const chunksRef = useRef([])
  const streamRef = useRef(null)
  const recognitionRef = useRef(null)
  const isRecordingRef = useRef(false)  // ğŸ”¥ Ref for onend callback closure
  const restartCountRef = useRef(0)  // ğŸ”¥ ì¬ì‹œì‘ íšŸìˆ˜ ì¶”ì 
  const maxRestarts = 5  // ğŸ”¥ ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
  const lastErrorRef = useRef(null)  // ğŸ”¥ ë§ˆì§€ë§‰ ì—ëŸ¬ ì €ì¥

  const { token } = useUserStore()

  // Initialize Web Speech API
  useEffect(() => {
    if (SpeechRecognition) {
      const recognition = new SpeechRecognition()
      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = 'ko-KR'  // Korean
      recognition.maxAlternatives = 1  // ğŸ”¥ ì„±ëŠ¥ ìµœì í™”

      recognition.onresult = (event) => {
        // ğŸ”¥ ê²°ê³¼ ìˆ˜ì‹  ì‹œ ì¬ì‹œì‘ ì¹´ìš´íŠ¸ ì´ˆê¸°í™” (ì •ìƒ ì‘ë™ ì¤‘)
        restartCountRef.current = 0
        
        let interimTranscript = ''
        let finalText = ''

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalText += transcript
          } else {
            interimTranscript += transcript
          }
        }

        if (finalText) {
          setFinalTranscript(prev => prev + finalText)
        }
        setLiveTranscript(interimTranscript)
      }

      recognition.onerror = (event) => {
        console.error('ğŸ”´ Speech recognition error:', event.error)
        lastErrorRef.current = event.error
        
        // ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
        switch (event.error) {
          case 'no-speech':
            // ìŒì„± ì—†ìŒ - ì •ìƒì ì¸ ìƒí™©, ìë™ ì¬ì‹œì‘ë¨
            console.log('ğŸ”‡ No speech detected, will auto-restart')
            break
          case 'aborted':
            // ì˜ë„ì  ì¤‘ë‹¨ - ë¬´ì‹œ
            break
          case 'network':
            // ğŸ”¥ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ - ì¬ì‹œë„ ê°€ëŠ¥ (3íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œì—ë§Œ ì—ëŸ¬ í‘œì‹œ)
            console.warn('ğŸŒ Network error, will retry automatically...')
            if (restartCountRef.current >= 3) {
              setError('ìŒì„± ì¸ì‹ ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
            }
            // ì¬ì‹œì‘ì€ onendì—ì„œ ì²˜ë¦¬ë¨
            break
          case 'not-allowed':
          case 'service-not-allowed':
            // ë§ˆì´í¬ ê¶Œí•œ ì—†ìŒ
            setError('ë§ˆì´í¬ ì‚¬ìš© ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.')
            break
          case 'audio-capture':
            // ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨
            setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            break
          default:
            setError(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${event.error}`)
        }
      }

      // ğŸ”¥ Auto-restart when recognition ends unexpectedly (ê°œì„ ëœ ë²„ì „)
      recognition.onend = () => {
        console.log('ğŸ¤ Speech recognition ended, isRecording:', isRecordingRef.current, 'restarts:', restartCountRef.current)
        
        // If still recording and within restart limit, auto-restart
        if (isRecordingRef.current && restartCountRef.current < maxRestarts) {
          // ì¹˜ëª…ì  ì—ëŸ¬ì¼ ê²½ìš° ì¬ì‹œì‘ ì•ˆí•¨
          if (['not-allowed', 'service-not-allowed', 'audio-capture'].includes(lastErrorRef.current)) {
            console.log('â›” Skipping restart due to critical error:', lastErrorRef.current)
            return
          }
          
          restartCountRef.current++
          console.log(`ğŸ”„ Auto-restarting speech recognition (${restartCountRef.current}/${maxRestarts})...`)
          
          // ğŸ”¥ abort() í›„ ë”œë ˆì´ë¥¼ ë‘ê³  start()
          setTimeout(() => {
            if (isRecordingRef.current && recognitionRef.current) {
              try {
                recognitionRef.current.abort()  // ğŸ”¥ ê¹”ë”í•˜ê²Œ ì •ë¦¬
              } catch (e) {
                // ignore abort error
              }
              
              // abort í›„ ì¶”ê°€ ë”œë ˆì´
              setTimeout(() => {
                if (isRecordingRef.current && recognitionRef.current) {
                  try {
                    recognitionRef.current.start()
                    console.log('âœ… Speech recognition restarted successfully')
                    lastErrorRef.current = null  // ì„±ê³µ ì‹œ ì—ëŸ¬ ì´ˆê¸°í™”
                  } catch (e) {
                    console.warn('Failed to restart speech recognition:', e.message)
                  }
                }
              }, 100)  // abort í›„ 100ms ì¶”ê°€ ëŒ€ê¸°
            }
          }, 300)  // ğŸ”¥ 300ms ë”œë ˆì´ (100ms â†’ 300ms ì¦ê°€)
        } else if (restartCountRef.current >= maxRestarts) {
          console.warn('âš ï¸ Max restart attempts reached, stopping auto-restart')
        }
      }

      recognitionRef.current = recognition
    } else {
      console.warn('Web Speech API not supported')
    }

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort()
        } catch (e) {
          // ignore
        }
      }
    }
  }, [])

  // Get combined transcript (final + live)
  const getCurrentTranscript = useCallback(() => {
    return (finalTranscript + liveTranscript).trim()
  }, [finalTranscript, liveTranscript])

  // Start recording (MediaRecorder + Web Speech API)
  // ğŸ”¥ Returns the stream so visualizer can use the same stream
  const startRecording = useCallback(async () => {
    try {
      setError(null)
      setResult(null)
      setLiveTranscript('')
      setFinalTranscript('')
      setAudioBlob(null)  // ğŸ”¥ ì´ì „ ì˜¤ë””ì˜¤ ì´ˆê¸°í™”!
      chunksRef.current = []
      restartCountRef.current = 0  // ğŸ”¥ ë…¹ìŒ ì‹œì‘ ì‹œ ì¬ì‹œì‘ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
      lastErrorRef.current = null  // ğŸ”¥ ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100,
        }
      })

      streamRef.current = stream

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: MediaRecorder.isTypeSupported('audio/webm')
          ? 'audio/webm'
          : 'audio/mp4',
      })

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: mediaRecorder.mimeType })
        setAudioBlob(blob)
      }

      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start(100)
      setIsRecording(true)
      isRecordingRef.current = true  // ğŸ”¥ Sync ref for onend callback

      // Start Web Speech API (Fast Track)
      if (recognitionRef.current) {
        try {
          recognitionRef.current.start()
        } catch (e) {
          console.warn('Speech recognition already started:', e)
        }
      }

      // ğŸ”¥ Return stream so visualizer can use the same stream
      return stream

    } catch (err) {
      console.error('Recording error:', err)
      setError('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.')
      return null
    }
  }, [])

  // Stop recording - returns Promise with audio blob
  const stopRecording = useCallback(() => {
    return new Promise((resolve) => {
      if (mediaRecorderRef.current && isRecording) {
        const mediaRecorder = mediaRecorderRef.current

        // ğŸ”¥ onstop ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ êµì²´í•˜ì—¬ í˜„ì¬ ë…¹ìŒ blobì„ ë°˜í™˜
        mediaRecorder.onstop = () => {
          const blob = new Blob(chunksRef.current, { type: mediaRecorder.mimeType })
          console.log('ğŸµ Recording stopped, blob size:', blob.size, 'chunks:', chunksRef.current.length)
          setAudioBlob(blob)
          resolve(blob)  // ë…¹ìŒ ì™„ë£Œ í›„ blob ë°˜í™˜
        }

        mediaRecorder.stop()
        setIsRecording(false)
        isRecordingRef.current = false  // ğŸ”¥ Sync ref to prevent auto-restart

        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop())
        }

        // Stop Web Speech API
        if (recognitionRef.current) {
          recognitionRef.current.stop()
        }
      } else {
        // ë…¹ìŒ ì¤‘ì´ ì•„ë‹ˆë©´ null ë°˜í™˜
        resolve(null)
      }
    })
  }, [isRecording])

  // Analyze voice (send to backend with STT text)
  // ğŸ”¥ audioBlobì„ ì§ì ‘ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ (stopRecordingì—ì„œ ë°˜í™˜ëœ blob)
  const analyzeVoice = useCallback(async (battleId, expectedSpell, characterId = 'char_001', providedBlob = null, isUltimate = false) => {
    setIsAnalyzing(true)
    setError(null)

    // Get final STT text
    const sttText = getCurrentTranscript()
    console.log('ğŸ“ STT Text:', sttText)

    // ğŸ”¥ ì œê³µëœ blob ì‚¬ìš©, ì—†ìœ¼ë©´ state/chunks fallback
    const currentBlob = providedBlob
      || audioBlob
      || (chunksRef.current.length > 0 ? new Blob(chunksRef.current, { type: 'audio/webm' }) : null)

    console.log('ğŸµ Using provided blob:', !!providedBlob, 'size:', currentBlob?.size)

    if (!currentBlob) {
      console.warn('No audio blob available, using demo mode')
      // Demo mode fallback
      const mockResult = generateMockResult(expectedSpell, sttText)
      setResult(mockResult)
      setIsAnalyzing(false)
      return mockResult
    }

    try {
      const formData = new FormData()
      formData.append('audio_file', currentBlob, 'voice.webm')
      formData.append('battle_id', battleId)
      formData.append('expected_spell', expectedSpell)
      formData.append('stt_text', sttText)  // Send Web Speech API result
      formData.append('character_id', characterId)
      formData.append('is_ultimate', isUltimate.toString())  // ê¶ê·¹ê¸° ì—¬ë¶€ ì „ë‹¬

      const headers = {}
      if (token) {
        headers['Authorization'] = `Bearer ${token}`
      }

      const response = await fetch(`${API_URL}/battle/voice-analyze`, {
        method: 'POST',
        headers,
        body: formData,
      })

      if (!response.ok) {
        throw new Error('Voice analysis failed')
      }

      const data = await response.json()
      console.log('ğŸ¯ Analysis result:', data)
      setResult(data)
      return data

    } catch (err) {
      console.error('Analysis error:', err)
      // Fallback to demo mode on error
      const mockResult = generateMockResult(expectedSpell, sttText)
      setResult(mockResult)
      return mockResult
    } finally {
      setIsAnalyzing(false)
    }
  }, [audioBlob, token, getCurrentTranscript])

  // Generate mock result for demo mode
  const generateMockResult = useCallback((expectedSpell, sttText) => {
    // Calculate accuracy based on actual STT result if available
    let accuracy
    if (sttText && sttText.length > 0) {
      // Simple similarity calculation
      const normalize = (s) => s.toLowerCase().replace(/\s/g, '').replace(/[!?]/g, '')
      const sttNorm = normalize(sttText)
      const expectedNorm = normalize(expectedSpell)
      const minLen = Math.min(sttNorm.length, expectedNorm.length)
      const maxLen = Math.max(sttNorm.length, expectedNorm.length)
      let matches = 0
      for (let i = 0; i < minLen; i++) {
        if (sttNorm[i] === expectedNorm[i]) matches++
      }
      accuracy = maxLen > 0 ? matches / maxLen : 0
    } else {
      accuracy = 0.3 + Math.random() * 0.3 // Low accuracy if no STT
    }

    const volume = 50 + Math.random() * 40
    const confidence = 0.5 + accuracy * 0.3 + Math.random() * 0.2
    const isCritical = Math.random() > 0.7

    const grades = ['SSS', 'S', 'A', 'B', 'C', 'F']
    const gradeIndex = Math.min(5, Math.floor((1 - accuracy) * 6))
    const grade = isCritical && gradeIndex > 0 ? grades[gradeIndex - 1] : grades[gradeIndex]

    const baseDamage = 50
    const cringeBonus = Math.floor(90 * accuracy * 0.5)
    const volumeBonus = Math.floor(30 * (volume / 100))
    const multiplier = 0.5 + accuracy * 0.5 + confidence * 0.2
    let totalDamage = Math.floor((baseDamage + cringeBonus + volumeBonus) * multiplier)
    if (isCritical) totalDamage = Math.floor(totalDamage * 1.5)

    return {
      success: true,
      transcription: sttText || expectedSpell.split(' ').slice(0, 2).join(' ') + '...',
      analysis: {
        text_accuracy: Math.round(accuracy * 100) / 100,
        volume_db: Math.round(volume * 10) / 10,
        pitch_variance: Math.round((0.02 + Math.random() * 0.06) * 1000) / 1000,
        confidence: Math.round(confidence * 100) / 100
      },
      damage: {
        base_damage: baseDamage,
        cringe_bonus: cringeBonus,
        volume_bonus: volumeBonus,
        accuracy_multiplier: Math.round(multiplier * 100) / 100,
        total_damage: totalDamage,
        is_critical: isCritical
      },
      grade,
      animation_trigger: grade === 'SSS' ? 'ultimate_attack' : grade === 'S' ? 'special_attack_02' : 'normal_attack_01',
      is_critical: isCritical,
      audio_url: null
    }
  }, [])

  // Reset state
  const reset = useCallback(() => {
    setAudioBlob(null)
    setResult(null)
    setError(null)
    setLiveTranscript('')
    setFinalTranscript('')
  }, [])

  return {
    isRecording,
    isAnalyzing,
    audioBlob,
    result,
    error,
    // Real-time transcript (Fast Track)
    liveTranscript: finalTranscript + liveTranscript,
    startRecording,
    stopRecording,
    analyzeVoice,
    reset,
  }
}
